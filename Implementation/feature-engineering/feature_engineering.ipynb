{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feature Engineering Pipeline\n",
                "\n",
                "This notebook implements three versions of feature engineering for the Basketball Motion Capture Data Challenge. Each version builds upon insights derived from our `exhaustive_analysis.ipynb`.\n",
                "\n",
                "## Setup\n",
                "Ensure `utils.py` is in the same directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import sys\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Add current directory to path to import utils\n",
                "sys.path.append(os.getcwd())\n",
                "from utils import load_data, DT, NUM_FRAMES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "DATA_DIR = '../../data'  # Adjust if needed\n",
                "OUTPUT_DIR = '../../data/processed'\n",
                "VAL_SIZE = 0.2\n",
                "RANDOM_SEED = 42\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data & Create Validation Split\n",
                "We split the provided training data into a Training set (80%) and a Validation set (20%) to evaluate our models locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading Data...\")\n",
                "df_full, X_full, kp_cols = load_data(DATA_DIR, train=True)\n",
                "df_test, X_test, _ = load_data(DATA_DIR, train=False)\n",
                "\n",
                "# Validation Split\n",
                "# We simply split randomly as the test set contains the same participants (Interpolation task)\n",
                "train_idx, val_idx = train_test_split(np.arange(len(df_full)), test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
                "\n",
                "df_train = df_full.iloc[train_idx].reset_index(drop=True)\n",
                "X_train = X_full[train_idx]\n",
                "df_val = df_full.iloc[val_idx].reset_index(drop=True)\n",
                "X_val = X_full[val_idx]\n",
                "\n",
                "print(f\"Train Stats: {len(df_train)} shots\")\n",
                "print(f\"Val Stats:   {len(df_val)} shots\")\n",
                "print(f\"Test Stats:  {len(df_test)} shots\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## V1: The \"Naive\" Baseline\n",
                "\n",
                "### Logic\n",
                "This approach treats the time-series data as a single massive bag of statistics. We flatten the time dimension by computing global aggregations (`mean`, `std`, `min`, `max`) for every single keypoint axis.\n",
                "\n",
                "### Source of Idea\n",
                "Standard Machine Learning practice. Before building complex temporal models, we must establish a **lower bound** of performance. If a complex model cannot beat this simple statistical aggregation, the complex model is flawed.\n",
                "\n",
                "### Features\n",
                "- **Input:** 240 Frames x 207 Coords\n",
                "- **Output:** 207 Coords * 4 Stats = 828 Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features_v1(X: np.ndarray, keypoint_cols: list) -> pd.DataFrame:\n",
                "    feature_names = []\n",
                "    features = []\n",
                "    \n",
                "    stats = ['mean', 'std', 'min', 'max']\n",
                "    for col in keypoint_cols:\n",
                "        for stat in stats:\n",
                "            feature_names.append(f\"{col}_{stat}\")\n",
                "            \n",
                "    print(f\"Extracting V1 features for {X.shape[0]} shots...\")\n",
                "    for i in range(X.shape[0]):\n",
                "        means = np.nanmean(X[i], axis=0)\n",
                "        stds = np.nanstd(X[i], axis=0)\n",
                "        mins = np.nanmin(X[i], axis=0)\n",
                "        maxs = np.nanmax(X[i], axis=0)\n",
                "        \n",
                "        shot_feats = np.stack([means, stds, mins, maxs], axis=0).T.flatten()\n",
                "        features.append(shot_feats)\n",
                "        \n",
                "    return pd.DataFrame(features, columns=feature_names)\n",
                "\n",
                "# Generate V1\n",
                "train_v1 = extract_features_v1(X_train, kp_cols)\n",
                "val_v1 = extract_features_v1(X_val, kp_cols)\n",
                "test_v1 = extract_features_v1(X_test, kp_cols)\n",
                "\n",
                "# Save\n",
                "os.makedirs(f\"{OUTPUT_DIR}/v1\", exist_ok=True)\n",
                "pd.concat([train_v1, df_train[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v1/train_v1.csv\", index=False)\n",
                "pd.concat([val_v1, df_val[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v1/val_v1.csv\", index=False)\n",
                "test_v1.to_csv(f\"{OUTPUT_DIR}/v1/test_v1.csv\", index=False)\n",
                "print(\"V1 Saved (Train, Val, Test).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## V2: The \"Windowed\" Approach (Data-Driven)\n",
                "\n",
                "### Logic\n",
                "Instead of looking at the whole shot, we focus on the **critical moments**. A basketball shot is determined by the mechanics immediately preceding the release. We isolate this \"Pre-Release\" window and the \"Follow-Through\" separately.\n",
                "\n",
                "### Source of Idea\n",
                "Derived from **Level 3** of our `exhaustive_analysis.ipynb`. \n",
                "1.  **Why Windows?** The analysis showed tremendous variance in the \"setup\" phase (dribbling) that is noise. \n",
                "2.  **Why Dynamic?** We found that different players release at different times. Hardcoding \"Frame 180\" fails for fast shooters. \n",
                "3.  **The Algorithm:** We use the specific detection logic validated in the analysis: **Max Wrist Velocity** is the robust marker for the moment the ball leaves the hand.\n",
                "\n",
                "### Features\n",
                "- **Dynamic Release Frame:** Calculated per-shot using `argmax(wrist_velocity)`.\n",
                "- **Pre-Release Stats:** Mean/Std of features in [Release - 50 frames, Release].\n",
                "- **Release Snapshot:** Exact Position & Velocity at the moment of release."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_release_frame(shot_data: np.ndarray, kp_cols: list) -> int:\n",
                "    try:\n",
                "        wrist_idx = -1\n",
                "        for i, col in enumerate(kp_cols):\n",
                "            if 'right_wrist_x' in col:\n",
                "                wrist_idx = i // 3\n",
                "                break\n",
                "        if wrist_idx == -1: return 180\n",
                "            \n",
                "        # Velocity Magnitude of Wrist\n",
                "        wrist_pos = shot_data[:, wrist_idx*3 : wrist_idx*3 + 3]\n",
                "        vel = np.gradient(wrist_pos, DT, axis=0)\n",
                "        vel_mag = np.linalg.norm(vel, axis=1)\n",
                "        \n",
                "        # Search 140-230\n",
                "        search_start, search_end = 140, 230\n",
                "        window_vel = vel_mag[search_start:search_end]\n",
                "        if len(window_vel) == 0: return 180\n",
                "            \n",
                "        return int(np.nanargmax(window_vel) + search_start)\n",
                "    except:\n",
                "        return 180\n",
                "\n",
                "def extract_features_v2(X: np.ndarray, keypoint_cols: list) -> pd.DataFrame:\n",
                "    features = []\n",
                "    feature_names = []\n",
                "    for col in keypoint_cols:\n",
                "        feature_names.extend([f\"{col}_pre_mean\", f\"{col}_pre_std\", f\"{col}_rel_pos\", f\"{col}_rel_vel\", f\"{col}_post_mean\"])\n",
                "    feature_names.append(\"release_frame\")\n",
                "    \n",
                "    print(f\"Extracting V2 features for {X.shape[0]} shots...\")\n",
                "    for i in range(X.shape[0]):\n",
                "        shot = X[i]\n",
                "        rel_frame = detect_release_frame(shot, keypoint_cols)\n",
                "        \n",
                "        pre_win = shot[max(0, rel_frame-50):rel_frame]\n",
                "        post_win = shot[rel_frame:min(NUM_FRAMES, rel_frame+40)]\n",
                "        vel_full = np.gradient(shot, DT, axis=0)\n",
                "        \n",
                "        row = []\n",
                "        for f_idx in range(len(keypoint_cols)):\n",
                "            p_mean = np.nanmean(pre_win[:, f_idx]) if len(pre_win) > 0 else 0\n",
                "            p_std = np.nanstd(pre_win[:, f_idx]) if len(pre_win) > 0 else 0\n",
                "            r_pos = shot[rel_frame, f_idx]\n",
                "            r_vel = vel_full[rel_frame, f_idx]\n",
                "            post_mean = np.nanmean(post_win[:, f_idx]) if len(post_win) > 0 else 0\n",
                "            row.extend([p_mean, p_std, r_pos, r_vel, post_mean])\n",
                "            \n",
                "        row.append(rel_frame)\n",
                "        features.append(row)\n",
                "        \n",
                "    return pd.DataFrame(features, columns=feature_names)\n",
                "\n",
                "# Generate V2\n",
                "train_v2 = extract_features_v2(X_train, kp_cols)\n",
                "val_v2 = extract_features_v2(X_val, kp_cols)\n",
                "test_v2 = extract_features_v2(X_test, kp_cols)\n",
                "\n",
                "# Save\n",
                "os.makedirs(f\"{OUTPUT_DIR}/v2\", exist_ok=True)\n",
                "pd.concat([train_v2, df_train[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v2/train_v2.csv\", index=False)\n",
                "pd.concat([val_v2, df_val[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v2/val_v2.csv\", index=False)\n",
                "test_v2.to_csv(f\"{OUTPUT_DIR}/v2/test_v2.csv\", index=False)\n",
                "print(\"V2 Saved (Train, Val, Test).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## V3: The \"Kinematic\" Approach (Physics-Based)\n",
                "\n",
                "### Logic\n",
                "This version transforms raw, noisy 3D coordinates into **biomechanical signals**. A player's position on the court (X,Y) shouldn't matter; their **form** (Joint Angles) does. We calculate the \"Kinetic Chain\": the sequential extension of Knee -> Elbow -> Wrist.\n",
                "\n",
                "### Source of Idea\n",
                "Domain Knowledge & Physics. \n",
                "- **Joint Angles:** The 'angle' target is directly correlated with the `Elbow Angle` and `Release Angle`. Raw X,Y,Z coords obscure this relationship.\n",
                "- **Smoothness:** A common coaching metric is \"fluidity\". We calculate 'Jerk' (change in acceleration) to quantify this.\n",
                "- **Projectile Motion:** The `depth` target is purely a function of `Release Velocity` and `Release Angle` (basic physics). We explicitly calculate these.\n",
                "\n",
                "### Features\n",
                "- **Joint Angles:** Knee, Elbow (computed via vector geometry).\n",
                "- **Kinetic Timing:** `Max Extension Velocity` of Knee vs Elbow (coordination).\n",
                "- **Release Physics:** Speed (m/s) and vertical height at release."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_angle(a, b, c): # b is vertex\n",
                "    ba = a - b\n",
                "    bc = c - b\n",
                "    cosine = np.einsum('ij,ij->i', ba, bc) / (np.linalg.norm(ba, axis=1) * np.linalg.norm(bc, axis=1) + 1e-10)\n",
                "    return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
                "\n",
                "def extract_features_v3(X: np.ndarray, keypoint_cols: list) -> pd.DataFrame:\n",
                "    def get_xyz(shot, name):\n",
                "        for i, c in enumerate(keypoint_cols):\n",
                "            if f\"{name}_x\" in c:\n",
                "                idx = i // 3\n",
                "                return shot[:, idx*3 : idx*3+3]\n",
                "        return None\n",
                "\n",
                "    features = []\n",
                "    print(f\"Extracting V3 features for {X.shape[0]} shots...\")\n",
                "    \n",
                "    for i in range(X.shape[0]):\n",
                "        shot = X[i]\n",
                "        # Joints\n",
                "        r_sh = get_xyz(shot, 'right_shoulder')\n",
                "        r_el = get_xyz(shot, 'right_elbow')\n",
                "        r_wr = get_xyz(shot, 'right_wrist')\n",
                "        r_hip = get_xyz(shot, 'right_hip')\n",
                "        r_kn = get_xyz(shot, 'right_knee')\n",
                "        r_an = get_xyz(shot, 'right_ankle')\n",
                "        \n",
                "        # Angles\n",
                "        elbow_ang = calculate_angle(r_sh, r_el, r_wr) if r_sh is not None else np.zeros(NUM_FRAMES)\n",
                "        knee_ang = calculate_angle(r_hip, r_kn, r_an) if r_hip is not None else np.zeros(NUM_FRAMES)\n",
                "        \n",
                "        # Metrics\n",
                "        min_knee = np.nanmin(knee_ang)\n",
                "        max_knee_ext_vel = np.nanmax(np.gradient(knee_ang, DT))\n",
                "        \n",
                "        # Release Physics (at max wrist velocity)\n",
                "        rel_params = [0, 0, 0]\n",
                "        if r_wr is not None:\n",
                "            wr_vel = np.gradient(r_wr, DT, axis=0)\n",
                "            wr_jerk = np.gradient(np.gradient(wr_vel, DT, axis=0), DT, axis=0)\n",
                "            smoothness = np.nanmean(np.linalg.norm(wr_jerk, axis=1))\n",
                "            \n",
                "            # Release frame by max vertical velocity (y-axis)\n",
                "            # Assuming Y is vertical or major axis of lift\n",
                "            try:\n",
                "                max_v_frame = np.nanargmax(wr_vel[:, 1]) \n",
                "                rel_speed = np.linalg.norm(wr_vel[max_v_frame])\n",
                "                rel_height = r_wr[max_v_frame, 1]\n",
                "                rel_params = [smoothness, rel_speed, rel_height]\n",
                "            except:\n",
                "                pass\n",
                "                \n",
                "        features.append([min_knee, max_knee_ext_vel] + rel_params)\n",
                "        \n",
                "    cols = ['min_knee_angle', 'max_knee_ext_vel', 'smoothness', 'release_speed', 'release_height']\n",
                "    return pd.DataFrame(features, columns=cols)\n",
                "\n",
                "# Generate V3\n",
                "train_v3 = extract_features_v3(X_train, kp_cols)\n",
                "val_v3 = extract_features_v3(X_val, kp_cols)\n",
                "test_v3 = extract_features_v3(X_test, kp_cols)\n",
                "\n",
                "# Save\n",
                "os.makedirs(f\"{OUTPUT_DIR}/v3\", exist_ok=True)\n",
                "pd.concat([train_v3, df_train[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v3/train_v3.csv\", index=False)\n",
                "pd.concat([val_v3, df_val[['angle', 'depth', 'left_right']]], axis=1).to_csv(f\"{OUTPUT_DIR}/v3/val_v3.csv\", index=False)\n",
                "test_v3.to_csv(f\"{OUTPUT_DIR}/v3/test_v3.csv\", index=False)\n",
                "print(\"V3 Saved (Train, Val, Test).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}